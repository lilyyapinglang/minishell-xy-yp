lexer_tokenize(input)
    ↓
初始化 lex:
    lex.input = input
    lex.pos = 0
    lex.quote = QUOTE_NONE
    lex.tokens = NULL
    ↓
进入 tokenize_loop()
    while input[pos] != '\0':
        skip_whitespace()

        if quote == NONE AND (operator OR fd-redir):
            add_operator_token()
        else:
            add_word_token()

    结束
    ↓
返回 token list

Bash do not have nasted quotes: just The most outside quote!

我拿着手指走过一行话。
空格我跳过。
如果符号到了，我剪一个符号词。
如果普通字，我剪一个普通词。
引号告诉我什么时候不要被空格吓停。
每剪好一个词，我放到篮子里。
走到头就结束了。
A 版本 = 用 pos 边走边检查逻辑 → 最后用 substring 截取词段

// 将 t_lexer 替换为您的 t_tokenize (在您提供的头文件中是 t_tokenize，这里保持与您代码中的 t_lexer 别名一致)
typedef struct s_tokenize t_lexer;
typedef struct s_tokenlist t_token; // token_create, token_add_back, token_list_free 保持不变
